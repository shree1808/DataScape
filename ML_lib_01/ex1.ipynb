{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Shree123\\\\spark\\\\spark-3.5.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext is the entry point for using Spark functionalities. It represents the connection to a Spark cluster and is used to create RDDs (Resilient Distributed Datasets) and perform operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('MyApp')\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Note: In Databricks, you can directly use spark for most operations, which implicitly uses SparkContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession is the unified entry point for reading data, managing configurations, and creating DataFrames. It encapsulates SparkContext and provides a higher-level API for working with structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('MyApp')    \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-FLEDSVSD:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2272f5aa2e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Crop_recommendation.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------+-----------+-----------+-----+\n",
      "|  N|  P|  K|temperature|   humidity|         ph|   rainfall|label|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+-----+\n",
      "| 90| 42| 43|20.87974371|82.00274423|6.502985292|202.9355362| rice|\n",
      "| 85| 58| 41|21.77046169|80.31964408|7.038096361|226.6555374| rice|\n",
      "| 60| 55| 44|23.00445915| 82.3207629|7.840207144|263.9642476| rice|\n",
      "| 74| 35| 40|26.49109635|80.15836264|6.980400905|242.8640342| rice|\n",
      "| 78| 42| 42|20.13017482|81.60487287|7.628472891|262.7173405| rice|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  N|  K|\n",
      "+---+---+\n",
      "| 90| 43|\n",
      "| 85| 41|\n",
      "| 60| 44|\n",
      "| 74| 40|\n",
      "| 78| 42|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('N', 'K').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering by Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------+-----------+-----------+-----+\n",
      "|  N|  P|  K|temperature|   humidity|         ph|   rainfall|label|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+-----+\n",
      "| 85| 58| 41|21.77046169|80.31964408|7.038096361|226.6555374| rice|\n",
      "| 60| 55| 44|23.00445915| 82.3207629|7.840207144|263.9642476| rice|\n",
      "| 69| 55| 38|22.70883798|82.63941394| 5.70080568|271.3248604| rice|\n",
      "| 94| 53| 40|20.27774362|82.89408619|5.718627178|241.9741949| rice|\n",
      "| 89| 54| 38|24.51588066| 83.5352163|6.685346424|230.4462359| rice|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+---+---+-----------+-----------+-----------+-----------+-----+\n",
      "|  N|  P|  K|temperature|   humidity|         ph|   rainfall|label|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+-----+\n",
      "| 34| 16| 25|30.07202564|50.96040505| 6.10729559|92.09609766|mango|\n",
      "| 33| 29| 34|31.40948821|49.21729127|6.832979509|92.99739415|mango|\n",
      "| 34| 38| 31|35.37775595|45.58110023|6.454045329|97.41586402|mango|\n",
      "| 31| 29| 26|28.22373428|47.40519056|5.024124684|97.76832322|mango|\n",
      "| 34| 34| 35|27.27433181|47.16808054|6.422710539|  95.257992|mango|\n",
      "| 33| 31| 34|31.32995611|50.22287593|5.421265283|89.78216168|mango|\n",
      "| 31| 36| 29|33.93679864|52.72170281|6.460542749| 97.4611918|mango|\n",
      "| 31| 20| 30|32.17752026|54.01352682|6.207495815|91.88766069|mango|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normal Filtering\n",
    "df.filter(col('P') > 50).show(5)\n",
    "\n",
    "\n",
    "# Filtering with multiple columns\n",
    "df.filter((col('N') > 30) & (col('N') < 35) & (col('label') == 'mango')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a New Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Amalgam|\n",
      "+-------+\n",
      "| 162540|\n",
      "| 202130|\n",
      "| 145200|\n",
      "+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('Amalgam', col('N') * col('P') * col('K'))\n",
    "\n",
    "df.select('Amalgam').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames: Higher-level API, optimized with Catalyst optimizer, and easier to use for SQL-like queries.\n",
    "\n",
    "RDDs: Lower-level API, more control over data, and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching and persisting data can improve performance by storing intermediate results.\n",
    "\n",
    "caching and persisting DataFrames are crucial techniques for optimizing performance, especially when dealing with large datasets and iterative algorithms. Both methods help to avoid recomputing the same DataFrame multiple times, thus saving time and resources.\n",
    "\n",
    "\n",
    "For Persistent \n",
    "\n",
    "Common Storage Levels:\n",
    "\n",
    "MEMORY_ONLY: Store the DataFrame only in memory. This is the default if you just call cache(). \n",
    "\n",
    "DISK_ONLY: Store the DataFrame only on disk.\n",
    "\n",
    "MEMORY_AND_DISK: Store the DataFrame in memory if there is enough space; otherwise, store it on disk.\n",
    "\n",
    "MEMORY_ONLY_SER: Store the DataFrame in memory as serialized objects (useful for reducing memory usage).\n",
    "\n",
    "DISK_ONLY_2: Store the DataFrame on disk with two replicas (useful for fault tolerance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------+-----------+-----------+-----+-------+\n",
      "|  N|  P|  K|temperature|   humidity|         ph|   rainfall|label|Amalgam|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+-----+-------+\n",
      "|  2| 40| 27|29.73770045|47.54885174|5.954626604|90.09586854|mango|   2160|\n",
      "| 39| 24| 31|33.55695561|53.72979826|4.757114897|98.67527561|mango|  29016|\n",
      "| 21| 26| 27|27.00315545|47.67525434|5.699586972|95.85118326|mango|  14742|\n",
      "| 25| 22| 25|33.56150184|45.53556603|5.977413803|95.70525913|mango|  13750|\n",
      "|  0| 21| 32|35.89855625|54.25964196|6.430139436|92.19721736|mango|      0|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# More on Filtering\n",
    "\n",
    "df.filter(col('label').isin('mango')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_data = [\n",
    "    (1, 'Alice', 101, '2022-01-15'),\n",
    "    (2, 'Bob', 102, '2023-03-12'),\n",
    "    (3, 'Cathy', 101, '2021-11-01'),\n",
    "    (4, 'David', 103, '2022-06-20'),\n",
    "    (5, 'Eve', 102, '2024-02-25')\n",
    "]\n",
    "employees_columns = [\"employee_id\", \"name\", \"department_id\", \"hire_date\"]\n",
    "employees_df = spark.createDataFrame(employees_data, employees_columns)\n",
    "\n",
    "departments_data = [\n",
    "    (101, 'Engineering'),\n",
    "    (102, 'Marketing'),\n",
    "    (103, 'HR')\n",
    "]\n",
    "departments_columns = [\"department_id\", \"department_name\"]\n",
    "departments_df = spark.createDataFrame(departments_data, departments_columns)\n",
    "\n",
    "# Create Salaries DataFrame\n",
    "salaries_data = [\n",
    "    (1, 70000, '2024-01-01'),\n",
    "    (1, 75000, '2024-06-01'),\n",
    "    (2, 80000, '2023-03-12'),\n",
    "    (3, 72000, '2024-01-01'),\n",
    "    (4, 68000, '2022-06-20'),\n",
    "    (5, 75000, '2024-02-25')\n",
    "]\n",
    "salaries_columns = [\"employee_id\", \"salary\", \"effective_date\"]\n",
    "salaries_df = spark.createDataFrame(salaries_data, salaries_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------+----------+----------+\n",
      "|employee_id| name|department_id| hire_date|row_number|\n",
      "+-----------+-----+-------------+----------+----------+\n",
      "|          3|Cathy|          101|2021-11-01|         1|\n",
      "|          2|  Bob|          102|2023-03-12|         1|\n",
      "|          4|David|          103|2022-06-20|         1|\n",
      "+-----------+-----+-------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# window function\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "# rank employee by hire date - row_number\n",
    "\n",
    "row_spec = Window.partitionBy(col('department_id')).orderBy(col('hire_date'))\n",
    "\n",
    "window_data = employees_df.withColumn('row_number', row_number().over(row_spec))\n",
    "window_data.filter(col('row_number') == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_and_pandas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
